# LIPNET-A-Lip-Reading-AI-Model
The goal of this project is to create a model capable of accurately transcribing spoken language by analyzing only the visual information of lip movements.

Utilizing the GRID dataset as the foundation for model training, an innovative system has been devised wherein animations derived from videos serve as input to a sophisticated neural network architecture. This model, implemented via Streamlit and TensorFlow, facilitates the generation of text annotations corresponding to the spoken content within the animations. The fusion of state-of-the-art machine learning techniques with seamless deployment via Streamlit underscores a convergence of cutting-edge methodologies for multimedia analysis and synthesis.  
